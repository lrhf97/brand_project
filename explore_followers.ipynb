{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import os\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\doria\\OneDrive\\Documents\\GitHub_Files\\brand_project\\data\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "case_path = path+\"\\data\"\n",
    "print(case_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>location</th>\n",
       "      <th>description</th>\n",
       "      <th>username</th>\n",
       "      <th>name</th>\n",
       "      <th>verified</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>following_count</th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>listed_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60637364</td>\n",
       "      <td>2009-07-27 16:16:11+00:00</td>\n",
       "      <td></td>\n",
       "      <td>Husband+Dad+Writer+Cyclist+Citizen</td>\n",
       "      <td>KaplanJonathanE</td>\n",
       "      <td>Jonathan Kaplan</td>\n",
       "      <td>False</td>\n",
       "      <td>1013</td>\n",
       "      <td>864</td>\n",
       "      <td>220</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>373160054</td>\n",
       "      <td>2011-09-14 02:59:38+00:00</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>kris_hanon</td>\n",
       "      <td>Kris Hanon</td>\n",
       "      <td>False</td>\n",
       "      <td>105</td>\n",
       "      <td>468</td>\n",
       "      <td>518</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1489298184534908930</td>\n",
       "      <td>2022-02-03 18:02:43+00:00</td>\n",
       "      <td></td>\n",
       "      <td>Husband, father, cyclist, student @NCSUgeospat...</td>\n",
       "      <td>AWalkerMorrison</td>\n",
       "      <td>Andrew Morrison</td>\n",
       "      <td>False</td>\n",
       "      <td>35</td>\n",
       "      <td>1445</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1523839034723872776</td>\n",
       "      <td>2022-05-10 01:35:39+00:00</td>\n",
       "      <td></td>\n",
       "      <td>Just a guy nothin fancy - A little bit of this...</td>\n",
       "      <td>Pedalpusher2941</td>\n",
       "      <td>Kyle</td>\n",
       "      <td>False</td>\n",
       "      <td>18</td>\n",
       "      <td>277</td>\n",
       "      <td>336</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49985899</td>\n",
       "      <td>2009-06-23 14:15:03+00:00</td>\n",
       "      <td></td>\n",
       "      <td>Dad, husband, paramedic, adaptive cyclist.</td>\n",
       "      <td>wykydgt</td>\n",
       "      <td>Daniel Williams</td>\n",
       "      <td>False</td>\n",
       "      <td>212</td>\n",
       "      <td>561</td>\n",
       "      <td>337</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                 created_at location  \\\n",
       "0             60637364  2009-07-27 16:16:11+00:00            \n",
       "1            373160054  2011-09-14 02:59:38+00:00            \n",
       "2  1489298184534908930  2022-02-03 18:02:43+00:00            \n",
       "3  1523839034723872776  2022-05-10 01:35:39+00:00            \n",
       "4             49985899  2009-06-23 14:15:03+00:00            \n",
       "\n",
       "                                         description         username  \\\n",
       "0                 Husband+Dad+Writer+Cyclist+Citizen  KaplanJonathanE   \n",
       "1                                                NaN       kris_hanon   \n",
       "2  Husband, father, cyclist, student @NCSUgeospat...  AWalkerMorrison   \n",
       "3  Just a guy nothin fancy - A little bit of this...  Pedalpusher2941   \n",
       "4         Dad, husband, paramedic, adaptive cyclist.          wykydgt   \n",
       "\n",
       "              name  verified  followers_count  following_count  tweet_count  \\\n",
       "0  Jonathan Kaplan     False             1013              864          220   \n",
       "1       Kris Hanon     False              105              468          518   \n",
       "2  Andrew Morrison     False               35             1445           43   \n",
       "3             Kyle     False               18              277          336   \n",
       "4  Daniel Williams     False              212              561          337   \n",
       "\n",
       "   listed_count  \n",
       "0            26  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             3  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "file_name = 'rs_fr_list.csv'\n",
    "df = pd.read_csv(case_path+\"\\\\\"+ file_name)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   id               150 non-null    int64 \n",
      " 1   created_at       150 non-null    object\n",
      " 2   location         150 non-null    object\n",
      " 3   description      123 non-null    object\n",
      " 4   username         150 non-null    object\n",
      " 5   name             150 non-null    object\n",
      " 6   verified         150 non-null    bool  \n",
      " 7   followers_count  150 non-null    int64 \n",
      " 8   following_count  150 non-null    int64 \n",
      " 9   tweet_count      150 non-null    int64 \n",
      " 10  listed_count     150 non-null    int64 \n",
      "dtypes: bool(1), int64(5), object(5)\n",
      "memory usage: 12.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>following_count</th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>listed_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.500000e+02</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.959003e+17</td>\n",
       "      <td>932.713333</td>\n",
       "      <td>1155.906667</td>\n",
       "      <td>5463.166667</td>\n",
       "      <td>27.093333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.117728e+17</td>\n",
       "      <td>3024.329485</td>\n",
       "      <td>1302.982981</td>\n",
       "      <td>10861.180443</td>\n",
       "      <td>102.344290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.527892e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.730083e+07</td>\n",
       "      <td>61.750000</td>\n",
       "      <td>322.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.586869e+08</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>718.000000</td>\n",
       "      <td>1444.500000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.473208e+17</td>\n",
       "      <td>452.000000</td>\n",
       "      <td>1482.500000</td>\n",
       "      <td>5173.250000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.529075e+18</td>\n",
       "      <td>24687.000000</td>\n",
       "      <td>9407.000000</td>\n",
       "      <td>75079.000000</td>\n",
       "      <td>981.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  followers_count  following_count   tweet_count  \\\n",
       "count  1.500000e+02       150.000000       150.000000    150.000000   \n",
       "mean   2.959003e+17       932.713333      1155.906667   5463.166667   \n",
       "std    5.117728e+17      3024.329485      1302.982981  10861.180443   \n",
       "min    6.527892e+06         0.000000        20.000000      0.000000   \n",
       "25%    3.730083e+07        61.750000       322.000000    222.000000   \n",
       "50%    3.586869e+08       183.000000       718.000000   1444.500000   \n",
       "75%    7.473208e+17       452.000000      1482.500000   5173.250000   \n",
       "max    1.529075e+18     24687.000000      9407.000000  75079.000000   \n",
       "\n",
       "       listed_count  \n",
       "count    150.000000  \n",
       "mean      27.093333  \n",
       "std      102.344290  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        3.000000  \n",
       "75%       11.000000  \n",
       "max      981.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>location</th>\n",
       "      <th>description</th>\n",
       "      <th>username</th>\n",
       "      <th>name</th>\n",
       "      <th>verified</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>following_count</th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>listed_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>20662440</td>\n",
       "      <td>2009-02-12 08:36:48+00:00</td>\n",
       "      <td></td>\n",
       "      <td>Bike nomad, boycotting fossil-fuels, working o...</td>\n",
       "      <td>philsturgeon</td>\n",
       "      <td>Tree Sturgeon üö¥üî•üåç</td>\n",
       "      <td>False</td>\n",
       "      <td>22769</td>\n",
       "      <td>3254</td>\n",
       "      <td>75079</td>\n",
       "      <td>981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>8931352</td>\n",
       "      <td>2007-09-17 14:38:55+00:00</td>\n",
       "      <td></td>\n",
       "      <td>contumacious, Madman.</td>\n",
       "      <td>SuaveRepublique</td>\n",
       "      <td>Jamie</td>\n",
       "      <td>False</td>\n",
       "      <td>2585</td>\n",
       "      <td>4817</td>\n",
       "      <td>61886</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>11013712</td>\n",
       "      <td>2007-12-10 09:18:35+00:00</td>\n",
       "      <td></td>\n",
       "      <td>All things #cycling</td>\n",
       "      <td>dfitzger</td>\n",
       "      <td>David Fitzgerald</td>\n",
       "      <td>False</td>\n",
       "      <td>1202</td>\n",
       "      <td>1350</td>\n",
       "      <td>45550</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>14608802</td>\n",
       "      <td>2008-05-01 01:33:17+00:00</td>\n",
       "      <td></td>\n",
       "      <td>Senior Director of Education Partnerships &amp; Le...</td>\n",
       "      <td>jasonklein</td>\n",
       "      <td>Jason Klein</td>\n",
       "      <td>False</td>\n",
       "      <td>3060</td>\n",
       "      <td>4999</td>\n",
       "      <td>38878</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>234540672</td>\n",
       "      <td>2011-01-05 22:33:41+00:00</td>\n",
       "      <td></td>\n",
       "      <td>Vermonter in NC, retired lowest-league soccer ...</td>\n",
       "      <td>keylor_halbur</td>\n",
       "      <td>Keylor Halbur</td>\n",
       "      <td>False</td>\n",
       "      <td>252</td>\n",
       "      <td>842</td>\n",
       "      <td>35033</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                 created_at location  \\\n",
       "60    20662440  2009-02-12 08:36:48+00:00            \n",
       "146    8931352  2007-09-17 14:38:55+00:00            \n",
       "74    11013712  2007-12-10 09:18:35+00:00            \n",
       "63    14608802  2008-05-01 01:33:17+00:00            \n",
       "27   234540672  2011-01-05 22:33:41+00:00            \n",
       "\n",
       "                                           description         username  \\\n",
       "60   Bike nomad, boycotting fossil-fuels, working o...     philsturgeon   \n",
       "146                              contumacious, Madman.  SuaveRepublique   \n",
       "74                                 All things #cycling         dfitzger   \n",
       "63   Senior Director of Education Partnerships & Le...       jasonklein   \n",
       "27   Vermonter in NC, retired lowest-league soccer ...    keylor_halbur   \n",
       "\n",
       "                  name  verified  followers_count  following_count  \\\n",
       "60   Tree Sturgeon üö¥üî•üåç     False            22769             3254   \n",
       "146              Jamie     False             2585             4817   \n",
       "74    David Fitzgerald     False             1202             1350   \n",
       "63         Jason Klein     False             3060             4999   \n",
       "27       Keylor Halbur     False              252              842   \n",
       "\n",
       "     tweet_count  listed_count  \n",
       "60         75079           981  \n",
       "146        61886            58  \n",
       "74         45550           113  \n",
       "63         38878           221  \n",
       "27         35033            11  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values([ 'tweet_count','followers_count'], ascending =False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['id','description','username']\n",
    "df[cols]\n",
    "\n",
    "key_words = ['bike','cycle']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = list(df['description'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Husband+Dad+Writer+Cyclist+Citizen',\n",
       " nan,\n",
       " 'Husband, father, cyclist, student @NCSUgeospatial, bike mechanic @rei',\n",
       " 'Just a guy nothin fancy - A little bit of this and a little bit of that. Ride bikes- The Office - Sports-  Over thinker w/ a touch of western.',\n",
       " 'Dad, husband, paramedic, adaptive cyclist.',\n",
       " '|Artist|Art Educator|Husband|Father|Cyclist|Hiker|',\n",
       " nan,\n",
       " 'Wife. ‚ù§Ô∏è Physician. üë©üèº\\u200d‚öïÔ∏è WVSOM Alumnaüë©\\u200düéì Women‚Äôs Health and LGBT üè≥Ô∏è\\u200düåà Advocate.',\n",
       " 'Reshaping global credit markets with @Credix_finance.',\n",
       " 'An amateur bicycle racing and touring team located in the Wash. DC Metro Area. Est 2013\\r\\n#phasecycling #pedalhardandstrongeveryday',\n",
       " 'Grey meow meow with a big floofy tail. He/Him. Cis. Probably NSFW',\n",
       " '\"Flip on your TV,\\r\\nAnd try to make sense out of that\"\\r\\n-Daniel Johnston, üé∂Life in Vainüé∂',\n",
       " 'Always look on the bright side of life!',\n",
       " 'Transplant surgeon, dialysis access surgeon. when I‚Äôm not on a bike. or cooking dinner.',\n",
       " 'Cyclist. Former Runner. Pug Lover. Oregon Duck.',\n",
       " 'Tweets are my own, RT do not equal endorsements. Black Lives Matter. #NCState #TransportationNerd #LetsGoCanes',\n",
       " 'üå±FBLC',\n",
       " 'trying to make the most of it.',\n",
       " nan,\n",
       " 'A.D.D. - Athlete, Dad, Designer',\n",
       " 'Be a decent human. Retweeter / Science / Social justice / Animals /Empathy #BLM #IStandWithUkraine',\n",
       " nan,\n",
       " nan,\n",
       " 'Nerd. Runner. Cycle from A to B. Father of two.',\n",
       " 'history PhD, Harvard-Westlake School, KidUnity co-dir, endurance athlete, father of 3 (any views expressed here are my own)',\n",
       " 'husband || father || ‚òïÔ∏è ‚ù§Ô∏è || healthy lifestyles || healthy city || wannabe photographer',\n",
       " 'i apologize in advance',\n",
       " 'Vermonter in NC, retired lowest-league soccer player, typo GOAT, cleanest but slowest cyclist ever paid by USPS',\n",
       " 'Chicago cat snuggler, bike rider, baby catcher, recovering improviser, loud laugher, nap taker. word nerd. Amazon in training. she/her',\n",
       " 'husband, father, cyclist, practitioner of the Idaho stop',\n",
       " 'Scientist, innovator, teacher, nerd, and overall badass. I strongly prefer facts, data, and science over conspiracy theories.',\n",
       " 'Just your normal guy who loves hockey, comics and cycling. He/Him',\n",
       " 'Principal and leadership coach: leadership for social justice and school & community partnerships',\n",
       " 'Y ahora que hacemos? cyclocross?',\n",
       " \"Dad...Husband...Gamer...Cyclist...Skier. Motto.....Don't be a dick.\",\n",
       " nan,\n",
       " 'An imperfect person loved by a perfect God and a pretty great wife. Very geeky about sports tech and two wheeled fun. Former RCEP and CSCS.\\r\\nLifelong Cyclone.',\n",
       " 'Take yourself on a ride with Bike Fun. Lessons, classes, encouragement & expertise for riders of all ages & abilities. Experience joy again. üö≤ üòç',\n",
       " nan,\n",
       " 'Historian, teacher, writer, slow cyclist, mediocre soccer coach. Books: Mainstreaming Fundamentalism (2021); Civil Religion and American Christianity (2020)',\n",
       " 'Single-Speed Mountain Biker |  1/6th IronMan | @PMBIAssociation Certified Instructor',\n",
       " '@moonlight_speed Super Late Builder | Real Life Racer | 2021 Motor Mile Speedway Track Champion | check out @H52Photography',\n",
       " 'Creating clarity from chaos | Dad, Husband | Python, Ruby, Rails, React | Process Lover | Getting a lil better each day | /G\\\\',\n",
       " nan,\n",
       " 'Brew Man',\n",
       " 'Simpsons did it.',\n",
       " 'Husband, dad, cyclist, InfoSec Professional and an ü¶ß, $BBIG, $SPY Options',\n",
       " 'Coowner of @moonlight_speed - trying to drive cars fast on @iracing for MLS and @lunatikracing',\n",
       " 'Fitness, Fellowship, & Faith coming to Morganton 9/25/21.  Free outdoor workouts for all men',\n",
       " nan,\n",
       " '@UChicago polisci grad student - IR, formal theory. PGH sports and blue devil basketball. Other interests include bikes, (coffee) beans, and beats',\n",
       " 'Caraque√±o',\n",
       " 'Rider of bikes, and drinker of beer. I like to be liked, but I‚Äôm kinda dumb.',\n",
       " 'Have a weakness for shoes? So do we.',\n",
       " 'Cycling event management & promotion (Dirty Kitten Productions) Alex & Chris Howell üö¥üèª\\u200d‚ôÄÔ∏èüò∫üö¥üèª\\u200d‚ôÇÔ∏è #dirtykittengravel #ridedirty',\n",
       " 'Digital-first but still love a commercial shoot with all the bells and whistles. President @findcurrency | Partner @entcanada | Ads for PM Harper',\n",
       " 'Group rides. Made easy. Manage or find a cycling group ride in your local area fast & easy!',\n",
       " 'he/him/his : Part Animal. Part Machine. All Nerd.',\n",
       " 'We Ride 4 is a non-profit cycling club full of active cyclist who support youth-based organizations. Visit our web site to learn more and become a member',\n",
       " 'Sun child of the Zodiac. Don‚Äôt bring any bullshit into my wheelhouse. Raised my daughters to be warriors. Raised my son to worship women.',\n",
       " 'Bike nomad, boycotting fossil-fuels, working on reforestation and ancient woodland restoration as co-founder of @ProtectEarthUK. he/him',\n",
       " '#CarolinaGirl #Momof2 #oneboy #onegirl #beachlife #OccupationalTherapist #LymphedemaTherapist #MelanomaSurvivor \\r\\n#TarHeels #UNCBBall #CarolinaPanthers',\n",
       " nan,\n",
       " 'Senior Director of Education Partnerships & Learning Solutions at  @P20Network & @NIUCOE @NIULive | Previous-Teacher, Principal, Asst Supt | (he/him)',\n",
       " 'In 27 minutes, enough solar energy strikes the Earth to meet all forms of energy usage in the world for a year.',\n",
       " 'Pastor, Husband of Sherree, Father to Zack, Meg, & Nate. Follower of Christ.',\n",
       " 'Cyclist. Prairie enthusiast. 2022 Kuat ambassador.',\n",
       " 'Director @PeterCullenPCT, Founder Aither Pty Ltd, Founder Alluvium Consulting',\n",
       " 'üÖûüÖ°üÖñüÖêüÖùüÖûüÖúüÖîüÖ£üÖêüÖõüÖõüÖòüÖíüÖ¢... Lecturer in Inorganic Chemistry at @UniofLeicester @LeicesterChem',\n",
       " 'Software engineer @growers_ . Retired Marine. Former racer. Huge Caniac for @canes! Husband to @ddclarkie. https://t.co/AbkJmg9XE5. Opinions are my own',\n",
       " '#tweetiatrician @UVA 2017, @EVMSedu 2022, PL1 @BearResident, @AmerMedicalAssn @AAPSOPT. He/Him üåà',\n",
       " 'Peds urologist in LA- Fellowship PD, LGBT+ representation in surgery. Views my own. Obsessive movie/TV/podcast fan. Podcast producer. üè≥Ô∏è\\u200düåàüë®üèº\\u200dü§ù\\u200düë®üèª',\n",
       " 'Married with kids. Ride @trekbikes running @shimanogravel using @wahoofitness wearing @ridgesupply socks and kit #traveltheworld #skithewest',\n",
       " 'Weichert Realtors\\r\\n121 N. Pitt St. Alexandria, VA 22314\\r\\nOffice - (703) 549- 8700\\r\\nDirect Line - (703) 936-7176\\r\\nHusband, Father, Trail Runner, Wino, fan of History',\n",
       " 'All things #cycling',\n",
       " \"I don't like complaining, but sometimes I have to.\",\n",
       " 'Cyclist for @BorntoBikeRT, does a bit of cooking occasionally. üá¨üáß',\n",
       " 'teacher | learner | Building a network of people invested in peddling possibility üåç | Follow me for tweets about teaching, leading and growing ideas',\n",
       " \"I'm a Systems Engineer. Passionate about Technology, Digital Marketing and Programming. Fascinated by Fashion and Makeup.\",\n",
       " '#VLSI ninja. @CvilleRacing club president. I made grilled cheese on the radiator.',\n",
       " '~ Adventurer and Chronicler of thoughts, finding my way through the vastness that is the world.... be it the WWW or that of reality and physicality?',\n",
       " 'Committed to building cycling in our communities near and far. @manukasport @Capitalortho1 @corona @mecuanywhere @hirobelLLC IG: VelovitRacing',\n",
       " nan,\n",
       " nan,\n",
       " 'Husband, Father of 3, Pastor of Lifepoint Church, mountain biker, apple addict, crossfitter, sneakerhead, and Friend of God!',\n",
       " 'Go to our Facebook and win a swanky prize: https://t.co/sRwfULT8Qe',\n",
       " 'Detroit, Michigan State, THFC, Music. Probably really mostly dogs if we‚Äôre being honest.',\n",
       " 'Pedalo cercando di non farmi schiacciare. Non sempre √® una metafora.',\n",
       " 'padres - bikes - cats',\n",
       " 'F1 Engineer and Crufts Flyball finalist',\n",
       " '2021 Welsh 10mTTChamp | 2013 Silver RTTC Circuit Series | 2018/2019/2021 Welsh Womens 3 Distance 25, 50 & 100m BAR Champ | Womens Team Comp Record 2018 25m TT',\n",
       " 'I‚Äôm just your average 51 year old gravel riding Dad. Follow me as I battle laziness, bad diet and a desk job, one kilometer at a time.',\n",
       " '‚òïÔ∏è | üöµ\\u200d‚ôÇÔ∏è | üì∑ |üç∫ I ride bikes!',\n",
       " nan,\n",
       " 'Life Begins Just Outside Your Comfort Zone. Husband, Endurance Junkie, Mobility Planner,  Business Owner - Send It!  #rouleur #IFB',\n",
       " 'We are a beer and bicycle social club.',\n",
       " 'Just a guy on a bike',\n",
       " 'Pronoun: ‚ÄòNot him again‚Äô BA Hons MPhil Criminology Ironman Manc Jew Zionist humanist SheffHallam/Cambridge MCFC ASMR BlackDog Epilepsy üáÆüá±üá¨üáßüáßüá∑üèä\\u200d‚ôÇÔ∏èüö¥üèº\\u200d‚ôÇÔ∏è',\n",
       " 'Agent, Lawyer, Husband, Father, Dog Owner',\n",
       " nan,\n",
       " 'God save the King ......',\n",
       " nan,\n",
       " 'Climbing Mountains and turning around \\r\\n.',\n",
       " nan,\n",
       " nan,\n",
       " 'Virginia Cyclocross Series information',\n",
       " nan,\n",
       " nan,\n",
       " 'father, husband, CEO, raised over 10,000 donations for Give Blood & DKMS. Rode all of The Tour De France & delivered over ¬£1.1m+ for Cure Leukaemia #Tour21 .',\n",
       " 'rides a bike.',\n",
       " 'Former boss of USGPcx and many other fun things. Bike Marketing, Ocean Loving. Adventure seeking. All opinions are strictly my own.',\n",
       " 'A Sikh who follows Wigan rugby league. Love snow and XC ski-ing.',\n",
       " nan,\n",
       " 'Author of What Goes Around (Faber, 2016) and Where There‚Äôs A Will (Profile, 2019). Lead Cyclist for @rideleloop. Usually hungry. She/her.',\n",
       " 'Family. 50K: X2; 26.2: X9; 30K: X3; 13.1: X14. Amb: @nuunhydration, @HoneyStinger, @PROcompression, @squirrelsnutbut, @AltraRunning, @ROADiD, #teamULTRA',\n",
       " 'Cyclist, football fan, music lover, family man.',\n",
       " 'You must be this tall to read this feed.',\n",
       " 'Father, dreamer, athlete, entrepreneur, audiophile, mediocre musician. @find_empathy',\n",
       " 'long toothed health care worker, self determination and sun.',\n",
       " 'Top 0.1% on https://t.co/j5xrCT922H.',\n",
       " 'Love cycling,family,holidays,hate being bored,cycling,other activities are available!!!!',\n",
       " nan,\n",
       " '\\U0001fae7',\n",
       " 'Healthy workplace advocate, passionate about üö¥\\u200d‚ôÄÔ∏è, health behaviour, üêß & ü¶¢. UCI Amateur World ITTü•à(2017)ü•â(2021). Opinions my own. Personal account.',\n",
       " nan,\n",
       " 'Dad to Jessica, cycling addict',\n",
       " 'Cabr√≥n, pero de buen coraz√≥n.',\n",
       " 'Incapaz de conducir con sandalias, lavar cubiertos y comer elotes',\n",
       " 'Film Colourist / NZ Born / Bikes / Coffee / He / Him / Dad / Cat Stepdad',\n",
       " 'Still looking for the orange one',\n",
       " 'Just a guy on a mission to find a cure for MS. #BikeMS #TeamMcCloy2022 #RSArsenal #NuunAmbassador #RidePactimo',\n",
       " \"Husband, Father, Programmer, Marine, Weightlifter, Hiker.\\r\\n\\r\\nJaguars fan since 1995, yeah yeah, i'm old.\",\n",
       " 'Rugby player + Air Ambulance = Retired. Cycling and running going better to date üòú',\n",
       " 'üá∫üá∏Husband-Dad-Brother-Believer Mtn. Bike & Triathlon Enthusiast. 40+ feelin‚Äô like 20-',\n",
       " nan,\n",
       " '\"A Y U N A N D O\"',\n",
       " 'Landscape architect, bicycle advocate, home brewer, road cyclist, explorer of life',\n",
       " nan,\n",
       " 'Lazer Sport North America\\r\\nBest Fit - Guaranteed or your money back!',\n",
       " 'Fugly ginger that loves family, friends, and big adventures!',\n",
       " 'Chartered Financial Planner, loves cycling. Views are my own.',\n",
       " 'Fundraising by turning old inner tubes, coffee sacks and other waste materials into gorgeous stuff and creating a cycle of good in the meantime.',\n",
       " 'Royal Navy veteran / @RNRMCA / One & All CC / Yorkshire Born / Cornish by marriage /',\n",
       " nan,\n",
       " nan,\n",
       " 'Wife of a Marine, Mom of amazing twin girls. Photographer, Director of customer service experience at #Photofy, Die hard Steelers Fan! #LetsGoPens Send Coffee!',\n",
       " 'contumacious, Madman.',\n",
       " 'AMERICAN üá∫üá∏ born n raised. It‚Äôs a Southern thing. USA üíØ things, things, things... üö¥üèºüöµüèºüéæüèãüèº\\u200d‚ôÄÔ∏èüõ´ #MAGA #ReElect45 supporter. Welcome to Trumplandia',\n",
       " '@nascarevolution (coordinator) - GLOBAL üìπü•à7Ô∏è‚É£0Ô∏è‚É£üõ¥ @the270montage_ The270montage@gmail.com üëª-gentry123 - @ctrl.alt.offical - DJ - Addy guy - SAV',\n",
       " nan]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "\n",
    "def deEmojify(inputString):\n",
    "    return inputString.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "\n",
    "for l in word_list:\n",
    "    #strip symbols\n",
    "    l = re.sub(r'[^\\w]', ' ', str(l))\n",
    "    l= deEmojify(l)\n",
    "    #lowercase\n",
    "    l = l.lower()\n",
    "\n",
    "word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize,word_tokenize \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Husband+Dad+Writer+Cyclist+Citizen',\n",
       " nan,\n",
       " 'Husband, father, cyclist, student @NCSUgeospatial, bike mechanic @rei',\n",
       " 'Just a guy nothin fancy - A little bit of this and a little bit of that. Ride bikes- The Office - Sports-  Over thinker w/ a touch of western.',\n",
       " 'Dad, husband, paramedic, adaptive cyclist.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\doria\\OneDrive\\Documents\\GitHub_Files\\brand_project\\explore_followers.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/doria/OneDrive/Documents/GitHub_Files/brand_project/explore_followers.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# sentences = nltk.Text(sent_tokenize(word_list))\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/doria/OneDrive/Documents/GitHub_Files/brand_project/explore_followers.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# print(\"# of Sentences:\",len(sentences))\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/doria/OneDrive/Documents/GitHub_Files/brand_project/explore_followers.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m words \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mText(word_tokenize(word_list))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/doria/OneDrive/Documents/GitHub_Files/brand_project/explore_followers.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m# of Words:\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39mlen\u001b[39m(words))\n",
      "File \u001b[1;32mc:\\Users\\doria\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\doria\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\nltk\\tokenize\\__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtokenizers/punkt/\u001b[39m\u001b[39m{\u001b[39;00mlanguage\u001b[39m}\u001b[39;00m\u001b[39m.pickle\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39;49mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\doria\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1276\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1272\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\u001b[39mself\u001b[39m, text, realign_boundaries\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   1273\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1274\u001b[0m \u001b[39m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[0;32m   1275\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msentences_from_text(text, realign_boundaries))\n",
      "File \u001b[1;32mc:\\Users\\doria\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentences_from_text\u001b[39m(\u001b[39mself\u001b[39m, text, realign_boundaries\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   1326\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m \u001b[39m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m \u001b[39m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m \u001b[39m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[39m    follows the period.\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1332\u001b[0m     \u001b[39mreturn\u001b[39;00m [text[s:e] \u001b[39mfor\u001b[39;00m s, e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32mc:\\Users\\doria\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentences_from_text\u001b[39m(\u001b[39mself\u001b[39m, text, realign_boundaries\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   1326\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m \u001b[39m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m \u001b[39m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m \u001b[39m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[39m    follows the period.\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1332\u001b[0m     \u001b[39mreturn\u001b[39;00m [text[s:e] \u001b[39mfor\u001b[39;00m s, e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32mc:\\Users\\doria\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1322\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[39mif\u001b[39;00m realign_boundaries:\n\u001b[0;32m   1321\u001b[0m     slices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[1;32m-> 1322\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m slices:\n\u001b[0;32m   1323\u001b[0m     \u001b[39myield\u001b[39;00m (sentence\u001b[39m.\u001b[39mstart, sentence\u001b[39m.\u001b[39mstop)\n",
      "File \u001b[1;32mc:\\Users\\doria\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1421\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1408\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m \u001b[39mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m \u001b[39mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1418\u001b[0m \u001b[39m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[0;32m   1419\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1420\u001b[0m realign \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1421\u001b[0m \u001b[39mfor\u001b[39;00m sentence1, sentence2 \u001b[39min\u001b[39;00m _pair_iter(slices):\n\u001b[0;32m   1422\u001b[0m     sentence1 \u001b[39m=\u001b[39m \u001b[39mslice\u001b[39m(sentence1\u001b[39m.\u001b[39mstart \u001b[39m+\u001b[39m realign, sentence1\u001b[39m.\u001b[39mstop)\n\u001b[0;32m   1423\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sentence2:\n",
      "File \u001b[1;32mc:\\Users\\doria\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\nltk\\tokenize\\punkt.py:318\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    316\u001b[0m iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(iterator)\n\u001b[0;32m    317\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     prev \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[0;32m    319\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    320\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\doria\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1393\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_slices_from_text\u001b[39m(\u001b[39mself\u001b[39m, text):\n\u001b[0;32m   1394\u001b[0m     last_break \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1395\u001b[0m     \u001b[39mfor\u001b[39;00m match, context \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_match_potential_end_contexts(text):\n\u001b[0;32m   1396\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[0;32m   1397\u001b[0m             \u001b[39myield\u001b[39;00m \u001b[39mslice\u001b[39m(last_break, match\u001b[39m.\u001b[39mend())\n",
      "File \u001b[1;32mc:\\Users\\doria\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1375\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1373\u001b[0m before_words \u001b[39m=\u001b[39m {}\n\u001b[0;32m   1374\u001b[0m matches \u001b[39m=\u001b[39m []\n\u001b[1;32m-> 1375\u001b[0m \u001b[39mfor\u001b[39;00m match \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lang_vars\u001b[39m.\u001b[39;49mperiod_context_re()\u001b[39m.\u001b[39;49mfinditer(text))):\n\u001b[0;32m   1376\u001b[0m     \u001b[39m# Ignore matches that have already been captured by matches to the right of this match\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m     \u001b[39mif\u001b[39;00m matches \u001b[39mand\u001b[39;00m match\u001b[39m.\u001b[39mend() \u001b[39m>\u001b[39m before_start:\n\u001b[0;32m   1378\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# sentences = nltk.Text(sent_tokenize(word_list))\n",
    "# print(\"# of Sentences:\",len(sentences))\n",
    "words = nltk.Text(word_tokenize(word_list))\n",
    "print(\"# of Words:\",len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2cfc9d8581d65022a209cb8b0a9e83688c16929e9355671653978702ca761102"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
